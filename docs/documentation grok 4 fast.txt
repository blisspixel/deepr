Grok 4 Fast
Pushing the Frontier of Cost-Efficient Intelligence








Abstract digital hummingbird
We're thrilled to present Grok 4 Fast, our latest advancement in cost-efficient reasoning models. Built on xAI’s learnings from Grok 4, Grok 4 Fast delivers frontier-level performance across Enterprise and Consumer domains—with exceptional token efficiency. This model pushes the boundaries for smaller and faster AI, making high-quality reasoning accessible to more users and developers. Grok 4 Fast features state-of-the-art (SOTA) cost-efficiency, cutting-edge web and X search capabilities, a 2M token context window, and a unified architecture that blends reasoning and non-reasoning modes in one model.

Advancing Cost-Efficient Intelligence
Grok 4 Fast sets a new frontier in cost-efficient intelligence, outperforming Grok 3 Mini across reasoning benchmarks while slashing token costs.

Benchmark pass@1	Grok 4 Fast	Grok 4	Grok 3 Mini (High)	GPT-5 (High)	GPT-5 Mini (High)
GPQA Diamond
85.7%	87.5%	79.0%	85.7%	82.3%
AIME 2025 (no tools)
92.0%	91.7%	83.0%	94.6%	91.1%
HMMT 2025 (no tools)
93.3%	90.0%	74.0%	93.3%	87.8%
HLE (no tools)
20.0%	25.4%	11.0%	24.8%	16.7%
LiveCodeBench (Jan-May)
80.0%	79.0%	70.0%	86.8%	77.4%
We used large-scale reinforcement learning to maximize the intelligence density of Grok 4 Fast. In our evaluations, Grok 4 Fast achieves comparable performance to Grok 4 on benchmarks while using 40% fewer thinking tokens on average.

Intelligence Density
Maximum performance at minimum cost

AIME 2024 (no tools)
Score
(%)
100%
Grok 4 Fast
Grok 4
Grok 3 Mini
Thinking tokens
28000
AIME 2025 (no tools)
Score
(%)
100%
Grok 4 Fast
Grok 4
Grok 3 Mini
Thinking tokens
28000
HMMT 2025 (no tools)
Score
(%)
100%
Grok 4 Fast
Grok 4
Grok 3 Mini
Thinking tokens
28000
GPQA Diamond
Score
(%)
100%
Grok 4 Fast
Grok 3 Mini
Grok 4
Thinking tokens
28000
This 40% increase in Grok 4 Fast's token efficiency, combined with a significantly lower price per token, results in a 98% reduction in price to achieve the same performance on frontier benchmarks as Grok 4. As verified by an independent review from Artificial Analysis, Grok 4 Fast exhibits a state-of-the-art (SOTA) price-to-intelligence ratio compared to other publicly available models on the Artificial Analysis Intelligence Index.

Intelligence vs. Price
Artificial Analysis Intelligence Index

Artificial Analysis Intelligence Index
75
$16
$32
$64
$128
$256
$512
$1,024
$2,048
$4,096
Grok 4
Grok 4 Fast
Grok 4 Fast (Non Reasoning)
GPT-4.1
GPT-5 (High)
GPT-5 (Minimal)
o3
Gemini 2.5 Flash (Thinking)
Gemini 2.5 Pro
Claude 4 Sonnet*
Claude 4.1 Opus*
DeepSeek V3.1
DeepSeek V3.1 (Thinking)
Kimi K2 0905
GLM-4.5
Llama 4 Maverick
Mistral Medium 3.1
47x cheaper
Cost to Run Intelligence Index
(USD, Log Scale)
*All Claude models were benchmarked with Extended Thinking.
Native Tool Use with SOTA Search
Grok 4 Fast was trained end-to-end with tool-use reinforcement learning (RL). It excels at deciding when to invoke tools like code execution or web browsing.

For instance, Grok 4 Fast exhibits frontier agentic search capabilities, seamlessly browsing the web and X to augment queries with real-time data. It hops through links, ingests media (including images and videos on X), and synthesizes findings at light speed.

Benchmark pass@1	Grok 4 Fast	Grok 4	Grok 3 (No Reasoning)
BrowseComp
44.9%	43.0%	—
SimpleQA
95.0%	94.0%	82.0%
Reka Research Eval
66.0%	58.0%	37.0%
BrowseComp (zh)
51.2%	45.0%	10.8%
X Bench Deepsearch (zh)
74.0%	66.0%	27.0%
X Browse*
58.0%	53.2%	20.8%
*X Browse is an internal benchmark evaluating agent's multihop search and browsing capabilities on X.
Frontier of General Post-training
Grok 4 Fast also establishes a new cost-effective frontier on general domain. We are excited to share Grok 4 Fast’s result on LMArena, where it has been privately battle-testing on the Search and Text Arenas.

In LMArena's Search Arena, grok-4-fast-search (code name: menlo) claims #1 with 1163 Elo — a commanding margin of 17 over o3-search. Its superior reasoning efficiency and intelligence density enable it to surpass much larger models on real-world, search-related tasks.

Artificial Intelligence Search Arena
In LMArena's Text Arena, grok-4-fast (code name: tahoe) ranks #8, performing on par with grok-4-0709 and highlighting its remarkable intelligence density. Notably, it significantly outperforms peers in its weight class, where all comparable size models rank 18th or below.

Artificial Intelligence Text Arena
Here are some examples of Grok 4 Fast in action:

Path of Exile
X Search
Research
What is the maximum number of experience points possible in Path of Exile 2?
Show entire trace
Thought for 24 seconds
Investigating game details
The request is about finding the maximum experience points in Path of Exile 2.
Path of Exile 2 is an action RPG, possibly still in early access as of September 10, 2025.
Searching for maximum experience points Path of Exile 2
reddit.com
polygon.com
turtlebeach.com
+2 others
The max level in Path of Exile 2 is 100, based on search results.
The question likely means the total XP needed to reach level 100.
Browsing https://www.poewiki.net/wiki/Experience
A recent search found the total XP to level 100 in Path of Exile is around 23 billion, per PoE Wiki.
Searching for total experience points to reach level 100 Path of Exile
reddit.com
pathofexile.fandom.com
pathofexile.com
+1 others
Refining search approach
Recent results suggest the total XP to level 100 might be over 4 billion, based on levels 91 and 97 data.
Browsing https://www.poewiki.net/wiki/Experience
Let's search for confirmation if PoE2 uses the same XP table as PoE1.
Searching for Path of Exile 2 experience requirements same as PoE1
reddit.com
pathofexile.com
pathofexile.fandom.com
+1 others
Earlier searches show max level 100 is the same for PoE1 and PoE2, with no mention of XP changes.
Searching for Path of Exile 2 total XP to level 100
reddit.com
pathofexile.com
pathofexile.fandom.com
+1 others
Since PoE2 shares the same game engine, it's likely the XP system is identical to PoE1.
Searching for Path of Exile 2 experience table level XP
pathofexile.fandom.com
reddit.com
poe2db.tw
+1 others
Found a site, PoE2DB, listing experience levels up to 100.
Browsing https://poe2db.tw/us/Experience
4,250,334,444

In Path of Exile 2, the maximum character level is 100, which requires a total of 4,250,334,444 experience points (XP) to reach from level 1. This value is the cumulative sum of XP needed for each level-up, based on the game's fixed experience table (identical to the original Path of Exile). After reaching level 100, additional XP can still be earned but does not grant further levels or meaningful progression.

To arrive at this total, the game's experience system uses a predefined table where each level requires progressively more XP. The "total XP" for level 100 is the sum of all XP thresholds from level 1 to 100. Here's how it's calculated step by step:

Understand the XP structure: Each level has an XP requirement to advance to the next (e.g., level 1 to 2 requires 525 XP). The total for a given level is the running sum of these requirements up to that point. Level 1 starts at 0 XP.
Reference the experience table: The official Path of Exile 2 data sources provide the full table. For example:
Level 2: 525 total XP (525 to gain from level 1).
Level 3: 1,760 total XP (1,235 to gain from level 2).
...and so on, up to level 100.
Sum the requirements: Add up the XP needed for all 99 level-ups (from 1 to 100). This can be computed programmatically or from the table's cumulative values, yielding 4,250,334,444.
Unified Model: Reasoning and Non-Reasoning
Previously, separate reasoning modes required distinct models. Grok 4 Fast introduces a unified architecture where reasoning (long chain-of-thought) and non-reasoning (quick responses) are handled by the same model weights, steered via system prompts. This unification reduces end-to-end latency as well as token costs, making Grok 4 Fast ideal for real-time applications.

In grok.com, this results in smooth transitions: responding instantly for simple queries or engaging in extended reasoning for complex ones. In the xAI API, developers can fine-tune this behavior, optimizing for speed or depth.

Grok 4 Fast in grok.com, iOS, and Android apps
Web Globe Icon
Open

Grok.com

iOS Play Store Icon
Open

Grok on iOS

Android Play Store Icon
Open

Grok on Android

Grok 4 Fast is available now for all users. In Fast and Auto modes, you will see a significant improvement in search and information seeking queries. Additionally, difficult queries in Auto mode will use Grok 4 Fast, which will provide a much faster experience without loss of quality. For the first time, all users, including free users, will have access to our latest model without restrictions, marking a step toward democratizing advanced AI.

Grok 4 Fast on OpenRouter, Vercel AI Gateway, and the xAI API
For a limited time, Grok 4 Fast will be available for free on OpenRouter and Vercel AI Gateway.

We're also rolling out Grok 4 Fast as two models: grok-4-fast-reasoning and grok-4-fast-non-reasoning, each with a 2M token context window. This allows developers to tune the amount of test-time compute applied to their use cases.

grok-4-fast-reasoning and grok-4-fast-non-reasoning are generally available via the xAI API according to the following pricing:

Token Type	<128k tokens	≥128k tokens
Input tokens	$0.20 / 1M	$0.40 / 1M
Output tokens	$0.50 / 1M	$1.00 / 1M
Cached input tokens	$0.05 / 1M
What's Next
We will continuously ship model improvements to Grok 4 Fast based on your feedback on x.com. Stay tuned for further integrations, including enhanced multimodal capabilities and agentic features.

Read the Grok 4 Fast model card here.

That's all for now - so long, and thanks for all the fish! The Hitchhiker's Guide to Grok
Welcome! In this guide, we'll walk you through the basics of using the xAI API.

Step 1: Create an xAI Account
First, you'll need to create an xAI account to access xAI API. Sign up for an account here.

Once you've created an account, you'll need to load it with credits to start using the API.

Step 2: Generate an API Key
Create an API key via the API Keys Page in the xAI API Console.

After generating an API key, we need to save it somewhere safe! We recommend you export it as an environment variable in your terminal or save it to a 
.env
 file.

Bash


export XAI_API_KEY="your_api_key"
Step 3: Make your first request
With your xAI API key exported as an environment variable, you're ready to make your first API request.

Let's test out the API using 
curl
. Paste the following directly into your terminal.

Bash


curl https://api.x.ai/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $XAI_API_KEY" \
-m 3600 \
-d '{
    "messages": [
        {
            "role": "system",
            "content": "You are Grok, a highly intelligent, helpful AI assistant."
        },
        {
            "role": "user",
            "content": "What is the meaning of life, the universe, and everything?"
        }
    ],
    "model": "grok-4",
    "stream": false
}'
Step 4: Make a request from Python or Javascript
As well as a native xAI Python SDK, the majority our APIs are fully compatible with the OpenAI and Anthropic SDKs. For example, we can make the same request from Python or Javascript like so:


Python

Javascript
Other

# In your terminal, first run:
# pip install xai-sdk
import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    timeout=3600, # Override default timeout with longer timeout for reasoning models
)
chat = client.chat.create(model="grok-4")
chat.append(system("You are Grok, a highly intelligent, helpful AI assistant."))
chat.append(user("What is the meaning of life, the universe, and everything?"))
response = chat.sample()
print(response.content)
Certain models also support Structured Outputs, which allows you to enforce a schema for the LLM output.

For an in-depth guide about using Grok for text responses, check out our Chat Guide.

Step 5: Use Grok to analyze images
Certain grok models can accept both text AND images as an input. For example:


Python

Javascript
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, image
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    timeout=3600, # Override default timeout with longer timeout for reasoning models
)
chat = client.chat.create(model="grok-4")
chat.append(
    user(
        "What's in this image?",
        image("https://science.nasa.gov/wp-content/uploads/2023/09/web-first-images-release.png")
    )
)
response = chat.sample()
print(response.content)
And voila! Grok will tell you exactly what's in the image:

This image is a photograph of a region in space, specifically a part of the Carina Nebula, captured by the James Webb Space Telescope. It showcases a stunning view of interstellar gas and dust, illuminated by young, hot stars. The bright points of light are stars, and the colorful clouds are composed of various gases and dust particles. The image highlights the intricate details and beauty of star formation within a nebula.

To learn how to use Grok vision for more advanced use cases, check out our Image Understanding Guide.

Monitoring usage
As you use your API key, you will be charged for the number of tokens used. For an overview, you can monitor your usage on the xAI Console Usage Page.

If you want a more granular, per request usage tracking, the API response includes a usage object that provides detail on prompt (input) and completion (output) token usage.

JSON


"usage": {
    "prompt_tokens":37,
    "completion_tokens":530,
    "total_tokens":800,
    "prompt_tokens_details": {
        "text_tokens":37,
        "audio_tokens":0,
        "image_tokens":0,
        "cached_tokens":8
    },
    "completion_tokens_details": {
        "reasoning_tokens":233,
        "audio_tokens":0,
        "accepted_prediction_tokens":0,
        "rejected_prediction_tokens":0
    },
    "num_sources_used":0
}
If you send requests too frequently or with long prompts, you might run into rate limits and get an error response. For more information, read Consumption and Rate Limits. Models and Pricing
An overview of our models' capabilities and their associated pricing.

Grok 4 Fast
We're excited to release grok-4-fast, our latest advancement in cost-efficient reasoning models.

Modalities

Context window

2,000,000

Features

Function calling

Structured outputs

Reasoning

Lightning fast

Low cost

View base model
View non-reasoning model
Grok 4 Fast
Model Pricing
Model
Modalities
Capabilities
Context
Rate limits
Pricing
Language models		Per million tokens
grok-code-fast-1







256,000	
2M
tpm
480
rpm

$0.20
$1.50
grok-4-fast-reasoning







2,000,000	
4M
tpm
480
rpm

$0.20
$0.50
grok-4-fast-non-reasoning







2,000,000	
4M
tpm
480
rpm

$0.20
$0.50
grok-4-0709







256,000	
2M
tpm
480
rpm

$3.00
$15.00
grok-3-mini







131,072	
480
rpm

$0.30
$0.50
grok-3







131,072	
600
rpm

$3.00
$15.00
grok-2-vision-1212us-east-1







32,768	
600
rpm

$2.00
$10.00
grok-2-vision-1212eu-west-1







32,768	
50
rps

$2.00
$10.00
Image generation models		Per image output
grok-2-image-1212







300
rpm

$0.07
Grok 4 Information for Grok 3 Users
When moving from 
grok-3
/
grok-3-mini
 to 
grok-4
, please note the following differences:

• Grok 4 is a reasoning model. There are no non-reasoning mode when using Grok 4.
• 
presencePenalty
, 
frequencyPenalty
 and 
stop
 parameters are not supported by reasoning models. Adding them in the request would result in error.
• Grok 4 does not have a 
reasoning_effort
 parameter. If a 
reasoning_effort
 is provided, the request will return error.
Tools Pricing
Requests which make use of xAI provided server-side tools are priced based on two components: token usage and server-side tool invocations. Since the agent autonomously decides how many tools to call, costs scale with query complexity.

Token Costs
All standard token types are billed at the rate for the model used in the request:

Input tokens: Your query and conversation history
Reasoning tokens: Agent's internal thinking and planning
Completion tokens: The final response
Image tokens: Visual content analysis (when applicable)
Cached prompt tokens: Prompt tokens that were served from cache rather than recomputed
Tool Invocation Costs
For a limited time, all server-side tool invocations are free until November 21, 2025.

Tool	Cost per 1,000 calls	Description
Web Search	$10	Internet search and page browsing
X Search	$10	X posts, users, and threads
Code Execution	$10	Python code execution environment
Document Search	$10	Search through uploaded files and documents
View Image	Token-based only	Image analysis within search results
View X Video	Token-based only	Video analysis within X posts
Collections Search	$2.50	Knowledge base search using xAI Collections
Remote MCP Tools	Token-based only	Custom MCP tools
For the view image and view x video tools, you will not be charged for the tool invocation itself but will be charged for the image tokens used to process the image or video.

For Remote MCP tools, you will not be charged for the tool invocation but will be charged for any tokens used.

For more information on using Tools, please visit our guide on Tools.

Live Search Pricing
The advanced agentic search capabilities powering grok.com are generally available in the new agentic tool calling API, and the Live Search API will be deprecated by December 15, 2025.

Live Search costs $25 per 1,000 sources requested, each source used (Web, X, News, RSS) in a request counts toward the usage. That means a search using 4 sources costs $0.10 while a search using 1 source is $0.025. A source (e.g. Web) may return multiple citations, but you will be charged for only one source.

The number of sources used can be found in the 
response
 object, which contains a field called 
response.usage.num_sources_used
.

For more information on using Live Search, visit our guide on Live Search or look for 
search_parameters
 parameter on API Reference - Chat Completions.

Documents Search Pricing
For users using our Collections API and Documents Search, the following pricing applies:

Item
Price
Documents Search
$2.50/1k requests
File Storage
free
Collections Storage
free
Usage Guidelines Violation Fee
A rare occurrence for most users, when your request is deemed to be in violation of our usage guideline by our system, we will charge a $0.05 per request usage guidelines violation fee.

Additional Information Regarding Models
No access to realtime events without Live Search enabled
Grok has no knowledge of current events or data beyond what was present in its training data.
To incorporate realtime data with your request, please use Live Search function, or pass any realtime data as context in your system prompt.
Chat models
No role order limitation: You can mix 
system
, 
user
, or 
assistant
 roles in any sequence for your conversation context.
Image input models
Maximum image size: 
20MiB
Maximum number of images: No limit
Supported image file types: 
jpg/jpeg
 or 
png
.
Any image/text input order is accepted (e.g. text prompt can precede image prompt)
The knowledge cut-off date of Grok 3 and Grok 4 are November, 2024.

Model Aliases
Some models have aliases to help user automatically migrate to the next version of the same model. In general:

<modelname>
 is aliased to the latest stable version.
<modelname>-latest
 is aliased to the latest version. This is suitable for users who want to access the latest features.
<modelname>-<date>
 refers directly to a specific model release. This will not be updated and is for workflows that demand consistency.
For most users, the aliased 
<modelname>
 or 
<modelname>-latest
 are recommended, as you would receive the latest features automatically.

Billing and Availability
Your model access might vary depending on various factors such as geographical location, account limitations, etc.

For how the bills are charged, visit Manage Billing for more information.

For the most up-to-date information on your team's model availability, visit Models Page on xAI Console.

Model Input and Output
Each model can have one or multiple input and output capabilities. The input capabilities refer to which type(s) of prompt can the model accept in the request message body. The output capabilities refer to which type(s) of completion will the model generate in the response message body.

This is a prompt example for models with 
text
 input capability:

JSON


[
  {
    "role": "system",
    "content": "You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy."
  },
  {
    "role": "user",
    "content": "What is the meaning of life, the universe, and everything?"
  }
]
This is a prompt example for models with 
text
 and 
image
 input capabilities:

JSON


[
  {
    "role": "user",
    "content": [
      {
        "type": "image_url",
        "image_url": {
          "url": "data:image/jpeg;base64,<base64_image_string>",
          "detail": "high"
        }
      },
      {
        "type": "text",
        "text": "Describe what's in this image."
      }
    ]
  }
]
This is a prompt example for models with 
text
 input and 
image
 output capabilities:

JSON


// The entire request body
{
  "model": "grok-4",
  "prompt": "A cat in a tree",
  "n": 4
}
Context Window
The context window determines the maximum amount of token accepted by the model in the prompt.

For more information on how token is counted, visit Consumption and Rate Limits.

If you are sending the entire conversation history in the prompt for use cases like chat assistant, the sum of all the prompts in your conversation history must be no greater than the context window.

Cached prompt tokens
Trying to run the same prompt multiple times? You can now use cached prompt tokens to incur less cost on repeated prompts. By reusing stored prompt data, you save on processing expenses for identical requests. Enable caching in your settings and start saving today!

The caching is automatically enabled for all requests without user input. You can view the cached prompt token consumption in the 
"usage"
 object. Stateful Response with Responses API
Vercel AI SDK Support: The Responses API is not yet supported in the Vercel AI SDK. Please use the xAI SDK or OpenAI SDK for this functionality.

Responses API is a new way of interacting with our models via API. It allows a stateful interaction with our models, where previous input prompts, reasoning content and model responses are saved by us. A user can continue the interaction by appending new prompt messages, rather than sending all of the previous messages.

Although you don't need to enter the conversation history in the request body, you will still be billed for the entire conversation history when using Responses API. The cost might be reduced as the conversation history might be automatically cached.

The responses will be stored for 30 days, after which they will be removed. If you want to continue a response after 30 days, please store your responses history as well as the encrypted thinking content to create a new response. The encrypted thinking content can then be sent in the request body to give you a better result. See Returning encrypted thinking content for more information on retrieving encrypted content.

Prerequisites
xAI Account: You need an xAI account to access the API.
API Key: Ensure that your API key has access to the chat endpoint and the chat model is enabled.
If you don't have these and are unsure of how to create one, follow the Hitchhiker's Guide to Grok.

You can create an API key on the xAI Console API Keys Page.

Set your API key in your environment:

Bash


export XAI_API_KEY="your_api_key"
Creating a new model response
The first step in using Responses API is analogous to using Chat Completions API. You will create a new response with prompts.

instructions
 parameter is currently not supported. The API will return an error if it is specified.

When sending images, it is advised to set 
store
 parameters to 
false
. Otherwise the request may fail.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
chat = client.chat.create(model="grok-4", store_messages=True)
chat.append(system("You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy."))
chat.append(user("What is the meaning of life, the universe, and everything?"))
response = chat.sample()
print(response)
# The response id that can be used to continue the conversation later
print(response.id)
If no system prompt is desired, for non-xAI SDK users, the request's input parameter can be simplified as a string user prompt:


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
chat = client.chat.create(model="grok-4", store_messages=True)
chat.append(user("What is 101*3"))
response = chat.sample()
print(response)
# The response id that can be used to continue the conversation later
print(response.id)
Returning encrypted thinking content
If you want to return the encrypted thinking traces, you need to specify 
use_encrypted_content=True
 in xAI SDK or gRPC request message, or 
include: ["reasoning.encrypted_content"]
 in the request body.

Modify the steps to create a chat client (xAI SDK) or change the request body as following:


Python
Other

chat = client.chat.create(model="grok-4",
        store_messages=True,
        use_encrypted_content=True)
See Adding encrypted thinking content on how to use the returned encrypted thinking content.

Chaining the conversation
We now have the 
id
 of the first response. With Chat Completions API, we typically send a stateless new request with all the previous messages.

With Responses API, we can send the 
id
 of the previous response, and the new messages to append to it.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
chat = client.chat.create(model="grok-4", store_messages=True)
chat.append(system("You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy."))
chat.append(user("What is the meaning of life, the universe, and everything?"))
response = chat.sample()
print(response)
# The response id that can be used to continue the conversation later
print(response.id)
# New steps
chat = client.chat.create(
    model="grok-4",
    previous_response_id=response.id,
    store_messages=True,
)
chat.append(user("What is the meaning of 42?"))
second_response = chat.sample()
print(second_response)
# The response id that can be used to continue the conversation later
print(second_response.id)
Adding encrypted thinking content
After returning the encrypted thinking content, you can also add it to a new response's input:


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
chat = client.chat.create(model="grok-4", store_messages=True, use_encrypted_content=True)
chat.append(system("You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy."))
chat.append(user("What is the meaning of life, the universe, and everything?"))
response = chat.sample()
print(response)
# The response id that can be used to continue the conversation later
print(response.id)
# New steps
chat.append(response)  ## Append the response and the SDK will automatically add the outputs from response to message history
chat.append(user("What is the meaning of 42?"))
second_response = chat.sample()
print(second_response)
# The response id that can be used to continue the conversation later
print(second_response.id)
Retrieving a previous model response
If you have a previous response's ID, you can retrieve the content of the response.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
response = client.chat.get_stored_completion("<The previous response's id>")
print(response)
Delete a model response
If you no longer want to store the previous model response, you can delete it.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user, system
client = Client(
    api_key=os.getenv("XAI_API_KEY"),
    management_api_key=os.getenv("XAI_MANAGEMENT_API_KEY"),
    timeout=3600,
)
response = client.chat.delete_stored_completion("<The previous response's id>")
print(response)
 Guides
Overview
The xAI API supports agentic server-side tool calling which enables the model to autonomously explore, search, and execute code to solve complex queries. Unlike traditional tool-calling where clients must handle each tool invocation themselves, xAI's agentic API manages the entire reasoning and tool-execution loop on the server side.

xAI Python SDK Users: Version 1.3.1 of the xai-sdk package is required to use the agentic tool calling API.

Tools Pricing
Agentic requests are priced based on two components: token usage and tool invocations. Since the agent autonomously decides how many tools to call, costs scale with query complexity.

For more details on Tools pricing, please check out the pricing page.

Agentic Tool Calling
When you provide server-side tools to a request, the xAI server orchestrates an autonomous reasoning loop rather than returning tool calls for you to execute. This creates a seamless experience where the model acts as an intelligent agent that researches, analyzes, and responds automatically.

Behind the scenes, the model follows an iterative reasoning process:

Analyzes the query and current context to determine what information is needed
Decides what to do next: Either make a tool call to gather more information or provide a final answer
If making a tool call: Selects the appropriate tool and parameters based on the reasoning
Executes the tool in real-time on the server and receives the results
Processes the tool response and integrates it with previous context and reasoning
Repeats the loop: Uses the new information to decide whether more research is needed or if a final answer can be provided
Returns the final response once the agent determines it has sufficient information to answer comprehensively
This autonomous orchestration enables complex multi-step research and analysis to happen automatically, with clients seeing the final result as well as optional real-time progress indicators like tool call notifications during streaming.

Core Capabilities
Web Search: Real-time search across the internet with the ability to both search the web and browse web pages.
X Search: Semantic and keyword search across X posts, users, and threads.
Code Execution: The model can write and execute Python code for calculations, data analysis, and complex computations.
Image/Video Understanding: Optional visual content understanding and analysis for search results encountered (video understanding is only available for X posts).
Collections Search: The model can search through your uploaded knowledge bases and collections to retrieve relevant information.
Remote MCP Tools: Connect to external MCP servers to access custom tools.
Document Search: Upload files and chat with them using intelligent document search. This tool is automatically enabled when you attach files to a chat message.
Quick Start
We strongly recommend using the xAI Python SDK in streaming mode when using agentic tool calling. Doing so grants you the full feature set of the API, including the ability to get real-time observability and immediate feedback during potentially long-running requests.

Here is a quick start example of using the agentic tool calling API.

Python


import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import web_search, x_search, code_execution
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    # All server-side tools active
    tools=[
        web_search(),
        x_search(),
        code_execution(),
    ],
)
# Feel free to change the query here to a question of your liking
chat.append(user("What are the latest updates from xAI?"))
is_thinking = True
for response, chunk in chat.stream():
    # View the server-side tool calls as they are being made in real-time
    for tool_call in chunk.tool_calls:
        print(f"\nCalling tool: {tool_call.function.name} with arguments: {tool_call.function.arguments}")
    if response.usage.reasoning_tokens and is_thinking:
        print(f"\rThinking... ({response.usage.reasoning_tokens} tokens)", end="", flush=True)
    if chunk.content and is_thinking:
        print("\n\nFinal Response:")
        is_thinking = False
    if chunk.content and not is_thinking:
        print(chunk.content, end="", flush=True)
print("\n\nCitations:")
print(response.citations)
print("\n\nUsage:")
print(response.usage)
print(response.server_side_tool_usage)
print("\n\nServer Side Tool Calls:")
print(response.tool_calls)
You will be able to see output like:

Output


Thinking... (270 tokens)
Calling tool: x_user_search with arguments: {"query":"xAI official","count":1}
Thinking... (348 tokens)
Calling tool: x_user_search with arguments: {"query":"xAI","count":5}
Thinking... (410 tokens)
Calling tool: x_keyword_search with arguments: {"query":"from:xai","limit":10,"mode":"Latest"}
Thinking... (667 tokens)
Calling tool: web_search with arguments: {"query":"xAI latest updates site:x.ai","num_results":5}
Thinking... (850 tokens)
Calling tool: browse_page with arguments: {"url": "https://x.ai/news"}
Thinking... (1215 tokens)
Final Response:
### Latest Updates from xAI (as of October 12, 2025)
xAI primarily shares real-time updates via their official X (Twitter) account (@xai), with more formal announcements on their website (x.ai). Below is a summary of the most recent developments...
... full response omitted for brevity
Citations:
[
'https://x.com/i/user/1912644073896206336',
'https://x.com/i/user/1019237602585645057',
'https://x.com/i/status/1975607901571199086',
'https://x.com/i/status/1975608122845896765',
'https://x.com/i/status/1975608070245175592',
'https://x.com/i/user/1603826710016819209',
'https://x.com/i/status/1975608007250829383',
'https://status.x.ai/',
'https://x.com/i/user/150543432',
'https://x.com/i/status/1975608184711880816',
'https://x.com/i/status/1971245659660718431',
'https://x.com/i/status/1975608132530544900',
'https://x.com/i/user/1661523610111193088',
'https://x.com/i/status/1977121515587223679',
'https://x.ai/news/grok-4-fast',
'https://x.com/i/status/1975608017396867282',
'https://x.ai/',
'https://x.com/i/status/1975607953391755740',
'https://x.com/i/user/1875560944044273665',
'https://x.ai/news',
'https://docs.x.ai/docs/release-notes'
]
Usage:
completion_tokens: 1216
prompt_tokens: 29137
total_tokens: 31568
prompt_text_tokens: 29137
reasoning_tokens: 1215
cached_prompt_text_tokens: 22565
server_side_tools_used: SERVER_SIDE_TOOL_X_SEARCH
server_side_tools_used: SERVER_SIDE_TOOL_X_SEARCH
server_side_tools_used: SERVER_SIDE_TOOL_X_SEARCH
server_side_tools_used: SERVER_SIDE_TOOL_WEB_SEARCH
server_side_tools_used: SERVER_SIDE_TOOL_WEB_SEARCH
{'SERVER_SIDE_TOOL_X_SEARCH': 3, 'SERVER_SIDE_TOOL_WEB_SEARCH': 2}
Server Side Tool Calls:
[id: "call_51132959"
function {
  name: "x_user_search"
  arguments: "{"query":"xAI official","count":1}"
}
, id: "call_00956753"
function {
  name: "x_user_search"
  arguments: "{"query":"xAI","count":5}"
}
, id: "call_07881908"
function {
  name: "x_keyword_search"
  arguments: "{"query":"from:xai","limit":10,"mode":"Latest"}"
}
, id: "call_43296276"
function {
  name: "web_search"
  arguments: "{"query":"xAI latest updates site:x.ai","num_results":5}"
}
, id: "call_70310550"
function {
  name: "browse_page"
  arguments: "{"url": "https://x.ai/news"}"
}
]
Understanding the Agentic Tool Calling Response
The agentic tool calling API provides rich observability into the autonomous research process. This section dives deep into the original code snippet above, covering key ways to effectively use the API and understand both real-time streaming responses and final results:

Real-time server-side tool calls
When executing agentic requests using streaming, you can observe every tool call decision the model makes in real-time via the 
tool_calls
 attribute on the 
chunk
 object. This shows the exact parameters the agent chose for each tool invocation, giving you visibility into its search strategy. Occasionally the model may decide to invoke multiple tools in parallel during a single turn, in which case each entry in the list of 
tool_calls
 would represent one of those parallel tool calls; otherwise, only a single entry would be present in 
tool_calls
.

Note: Only the tool call invocations themselves are shown - server-side tool call outputs are not returned in the API response. The agent uses these outputs internally to formulate its final response, but they are not exposed to the user.

When using the xAI Python SDK in streaming mode, it will automatically accumulate the 
tool_calls
 into the 
response
 object for you, letting you access a final list of all the server-side tool calls made during the agentic loop. This is demonstrated in the section below.

Python


for tool_call in chunk.tool_calls:
    print(f"\nCalling tool: {tool_call.function.name} with arguments: {tool_call.function.arguments}")
Output


Calling tool: x_user_search with arguments: {"query":"xAI official","count":1}
Calling tool: x_user_search with arguments: {"query":"xAI","count":5}
Calling tool: x_keyword_search with arguments: {"query":"from:xai","limit":10,"mode":"Latest"}
Calling tool: web_search with arguments: {"query":"xAI latest updates site:x.ai","num_results":5}
Calling tool: browse_page with arguments: {"url": "https://x.ai/news"}
Citations
The 
citations
 attribute on the 
response
 object provides a comprehensive list of URLs for all sources the agent encountered during its search process. They are only returned when the agentic request completes and are not available in real-time during streaming. Citations are automatically collected from successful tool executions and provide full traceability of the agent's information sources.

Note that not every URL here will necessarily be relevant to the final answer, as the agent may examine a particular source and determine it is not sufficiently relevant to the user's original query.

Python


response.citations
Output


[
'https://x.com/i/user/1912644073896206336',
'https://x.com/i/status/1975607901571199086',
'https://x.ai/news',
'https://docs.x.ai/docs/release-notes',
...
]
Server-side Tool Calls vs Tool Usage
The API provides two related but distinct metrics for server-side tool executions:

tool_calls
 - All Attempted Calls

Python


response.tool_calls
Returns a list of all attempted tool calls made during the agentic process. Each entry is a ToolCall object containing:

id
: Unique identifier for the tool call
function.name
: The name of the specific server-side tool called
function.arguments
: The parameters passed to the server-side tool
This includes every tool call attempt, even if some fail.

Output


[id: "call_51132959"
function {
  name: "x_user_search"
  arguments: "{"query":"xAI official","count":1}"
}
, id: "call_07881908"
function {
  name: "x_keyword_search"
  arguments: "{"query":"from:xai","limit":10,"mode":"Latest"}"
}
, id: "call_43296276"
function {
  name: "web_search"
  arguments: "{"query":"xAI latest updates site:x.ai","num_results":5}"
}
]
server_side_tool_usage
 - Successful Calls (Billable)

Python


response.server_side_tool_usage
Returns a map of successfully executed tools and their invocation counts. This represents only the tool calls that returned meaningful responses and is what determines your billing.

Output


{'SERVER_SIDE_TOOL_X_SEARCH': 3, 'SERVER_SIDE_TOOL_WEB_SEARCH': 2}
Tool Call Function Names vs Usage Categories
The function names in 
tool_calls
 represent the precise/exact name of the tool invoked by the model, while the entries in 
server_side_tool_usage
 provide a more high-level categorization that aligns with the original tool passed in the 
tools
 array of the request.

Function Name to Usage Category Mapping:

Usage Category	Function Name(s)
SERVER_SIDE_TOOL_WEB_SEARCH
web_search
, 
web_search_with_snippets
, 
browse_page
SERVER_SIDE_TOOL_X_SEARCH
x_user_search
, 
x_keyword_search
, 
x_semantic_search
, 
x_thread_fetch
SERVER_SIDE_TOOL_CODE_EXECUTION
code_execution
SERVER_SIDE_TOOL_VIEW_X_VIDEO
view_x_video
SERVER_SIDE_TOOL_VIEW_IMAGE
view_image
SERVER_SIDE_TOOL_COLLECTIONS_SEARCH
collections_search
SERVER_SIDE_TOOL_MCP
{server_label}.{tool_name}
 if 
server_label
 provided, otherwise 
{tool_name}
When Tool Calls and Usage Differ
In most cases, 
tool_calls
 and 
server_side_tool_usage
 will show the same tools. However, they can differ when:

Failed tool executions: The model attempts to browse a non-existent webpage, fetch a deleted X post, or encounters other execution errors
Invalid parameters: Tool calls with malformed arguments that can't be processed
Network or service issues: Temporary failures in the tool execution pipeline
The agentic system is robust enough to handle these failures gracefully, updating its trajectory and continuing with alternative approaches when needed.

Billing Note: Only successful tool executions (
server_side_tool_usage
) are billed. Failed attempts are not charged.

Server-side Tool Call and Client-side Tool Call
Agentic tool calling supports mixing server-side tools and client-side tools, which enables more use cases when some private tools and data are needed during the agentic tool calling process.

To determine whether the received tool calls need to be executed by the client side, you can simply check the type of the tool call.

For xAI Python SDK users, you can use the provided 
get_tool_call_type
 function to get the type of the tool calls.

For a full guide into requests that mix server-side and client-side tools, please check out the advanced usage page.

xAI Python SDK Users: Version 1.4.0 of the xai-sdk package is the minimum requirement to use the 
get_tool_call_type
 function.

Python


# ...
response = chat.sample()
from xai_sdk.tools import get_tool_call_type
for tool_call in response.tool_calls:
    print(get_tool_call_type(tool_call))
The available tool call types are listed below:

Tool call types	Description
"client_side_tool"
Indicates this tool call is a client-side tool call, and an invocation to this function on the client side is required and the tool output needs to be appended to the chat
"web_search_tool"
Indicates this tool call is a web-search tool call, which is performed by xAI server, NO action needed from the client side
"x_search_tool"
Indicates this tool call is an x-search tool call, which is performed by xAI server, NO action needed from the client side
"code_execution_tool"
Indicates this tool call is a code-execution tool call, which is performed by xAI server, NO action needed from the client side
"collections_search_tool"
Indicates this tool call is a collections-search tool call, which is performed by xAI server, NO action needed from the client side
"mcp_tool"
Indicates this tool call is an MCP tool call, which is performed by xAI server, NO action needed from the client side
Understanding Token Usage
Agentic requests have unique token usage patterns compared to standard chat completions. Here's how each token field in the usage object is calculated:

completion_tokens
Represents only the final text output of the model - the comprehensive answer returned to the user. This is typically much smaller than you might expect for such rich, research-driven responses, as the agent performs all its intermediate reasoning and tool orchestration internally.

prompt_tokens
Represents the cumulative input tokens across all inference requests made during the agentic process. Since agentic workflows involve multiple reasoning steps with tool calls, the model makes several inference requests throughout the research process. Each request includes the full conversation history up to that point, which grows as the agent progresses through its research.

While this can result in higher 
prompt_tokens
 counts, agentic requests benefit significantly from prompt caching. The majority of the prompt (the conversation prefix) remains unchanged between inference steps, allowing for efficient caching of the shared context. This means that while the total 
prompt_tokens
 may appear high, much of the computation is optimized through intelligent caching of the stable conversation history, leading to better cost efficiency overall.

reasoning_tokens
Represents the tokens used for the model's internal reasoning process during agentic workflows. This includes the computational work the agent performs to plan tool calls, analyze results, and formulate responses, but excludes the final output tokens.

cached_prompt_text_tokens
Indicates how many prompt tokens were served from cache rather than recomputed. This shows the efficiency gains from prompt caching - higher values indicate better cache utilization and lower costs.

prompt_image_tokens
Represents the tokens derived from visual content that the agent processes during the request. These tokens are produced when visual understanding is enabled and the agent views images (e.g., via web browsing) or analyzes video frames on X. They are counted separately from text tokens and reflect the cost of ingesting visual features alongside the textual context. If no images or videos are processed, this value will be zero.

prompt_text_tokens
 and 
total_tokens
prompt_text_tokens
 reflects the actual text tokens in prompts (excluding any special tokens), while 
total_tokens
 is the sum of all token types used in the request.

Synchronous Agentic Requests (Non-streaming)
Although not typically recommended, for simpler use cases or when you want to wait for the complete agentic workflow to finish before processing the response, you can use synchronous requests:

Python


import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import code_execution, web_search, x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        web_search(),
        x_search(),
        code_execution(),
    ],
)
chat.append(user("What is the latest update from xAI?"))
# Get the final response in one go once it's ready
response = chat.sample()
print("\n\nFinal Response:")
print(response.content)
# Access the citations of the final response
print("\n\nCitations:")
print(response.citations)
# Access the usage details from the entire search process
print("\n\nUsage:")
print(response.usage)
print(response.server_side_tool_usage)
# Access the server side tool calls of the final response
print("\n\nServer Side Tool Calls:")
print(response.tool_calls)
Synchronous requests will wait for the entire agentic process to complete before returning the response. This is simpler for basic use cases but provides less visibility into the intermediate steps compared to streaming.

Using Tools with OpenAI Responses API
We also support using the OpenAI Responses API in both streaming and non-streaming modes.

Python

import os
import requests
url = "https://api.x.ai/v1/responses"
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {os.getenv('XAI_API_KEY')}"
}
payload = {
    "model": "grok-4-fast",
    "input": [
        {
            "role": "user",
            "content": "what is the latest update from xAI?"
        }
    ],
    "tools": [
        {
            "type": "web_search"
        },
        {
            "type": "x_search"
        }
    ]
}
response = requests.post(url, headers=headers, json=payload)
print(response.json())
Identifying the Client-side Tool Call
A critical step in mixing server-side tools and client-side tools is to identify whether a returned tool call is a client-side tool that needs to be executed locally on the client side.

Similar to the way in xAI Python SDK, you can identify the client-side tool call by checking the 
type
 of the output entries (
response.output[].type
) in the response of OpenAI Responses API.

Types	Description
"function_call"
Indicates this tool call is a client-side tool call, and an invocation to this function on the client side is required and the tool output needs to be appended to the chat
"web_search_call"
Indicates this tool call is a web-search tool call, which is performed by xAI server, NO action needed from the client side
"x_search_call"
Indicates this tool call is an x-search tool call, which is performed by xAI server, NO action needed from the client side
"code_interpreter_call"
Indicates this tool call is a code-execution tool call, which is performed by xAI server, NO action needed from the client side
"file_search_call"
Indicates this tool call is a collections-search tool call, which is performed by xAI server, NO action needed from the client side
"mcp_call"
Indicates this tool call is an MCP tool call, which is performed by xAI server, NO action needed from the client side
Agentic Tool Calling Requirements and Limitations
Model Compatibility
Supported Models: 
grok-4
, 
grok-4-fast
, 
grok-4-fast-non-reasoning
Strongly Recommended: 
grok-4-fast
 - specifically trained to excel at agentic tool calling
Request Constraints
No batch requests: 
n > 1
 not supported
No response format: Structured output not yet available with agentic tool calling
Limited sampling params: Only 
temperature
 and 
top_p
 are respected
Note: These constraints may be relaxed in future releases based on user feedback.

FAQ and Troubleshooting
I'm seeing empty or incorrect content when using agentic tool calling with the xAI Python SDK
Please make sure to upgrade to the latest version of the xAI SDK. Agentic tool calling requires version 
1.3.1
 or above. Search Tools
Agentic search represents one of the most compelling applications of agentic tool calling, with 
grok-4-fast
 specifically trained to excel in this domain. Leveraging its speed and reasoning capabilities, the model iteratively calls search tools—analyzing responses and making follow-up queries as needed—to seamlessly navigate web pages and X posts, uncovering difficult-to-find information or insights that would otherwise require extensive human analysis.

xAI Python SDK Users: Version 1.3.1 of the xai-sdk package is required to use the agentic tool calling API.

Available Search Tools
You can use the following server-side search tools in your request:

Web Search - allows the agent to search the web and browse pages
X Search - allows the agent to perform keyword search, semantic search, user search, and thread fetch on X
You can customize which tools are enabled in a given request by listing the needed tools in the 
tools
 parameter in the request.

Tool	xAI SDK	OpenAI Responses API
Web Search	
web_search
web_search
X Search	
x_search
x_search
Retrieving Citations
Citations provide traceability for sources used during agentic search. Access them from the response object:


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import web_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[web_search()],
)
chat.append(user("What is xAI?"))
is_thinking = True
for response, chunk in chat.stream():
    # View the server-side tool calls as they are being made in real-time
    for tool_call in chunk.tool_calls:
        print(f"\nCalling tool: {tool_call.function.name} with arguments: {tool_call.function.arguments}")
    if response.usage.reasoning_tokens and is_thinking:
        print(f"\rThinking... ({response.usage.reasoning_tokens} tokens)", end="", flush=True)
    if chunk.content and is_thinking:
        print("\n\nFinal Response:")
        is_thinking = False
    if chunk.content and not is_thinking:
        print(chunk.content, end="", flush=True)
print("\n\nCitations:")
print(response.citations)
print("\n\nUsage:")
print(response.usage)
print(response.server_side_tool_usage)
print("\n\nServer Side Tool Calls:")
print(response.tool_calls)
As mentioned in the overview page, the citations array contains the URLs of all sources the agent encountered during its search process, meaning that not every URL here will necessarily be relevant to the final answer, as the agent may examine a particular source and determine it is not sufficiently relevant to the user's original query.

For complete details on citations, including when they're available and usage notes, see the overview page.

Applying Search Filters to Control Agentic Search
Each search tool supports a set of optional search parameters to help you narrow down the search space and limit the sources/information the agent is exposed to during its search process.

Tool	Supported Filter Parameters
Web Search	
allowed_domains
, 
excluded_domains
, 
enable_image_understanding
X Search	
allowed_x_handles
, 
excluded_x_handles
, 
from_date
, 
to_date
, 
enable_image_understanding
, 
enable_video_understanding
Web Search Parameters
Only Search in Specific Domains
Use 
allowed_domains
 to make the web search only perform the search and web browsing on web pages that fall within the specified domains.

allowed_domains
 can include a maximum of five domains.

allowed_domains
 cannot be set together with 
excluded_domains
 in the same request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import web_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        web_search(allowed_domains=["wikipedia.org"]),
    ],
)
chat.append(user("What is xAI?"))
# stream or sample the response...
Exclude Specific Domains
Use 
excluded_domains
 to prevent the model from including the specified domains in any web search tool invocations and from browsing any pages on those domains.

excluded_domains
 can include a maximum of five domains.

excluded_domains
 cannot be set together with 
allowed_domains
 in the same request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import web_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        web_search(excluded_domains=["wikipedia.org"]),
    ],
)
chat.append(user("What is xAI?"))
# stream or sample the response...
Enable Image Understanding
Setting 
enable_image_understanding
 to true equips the agent with access to the 
view_image
 tool, allowing it to invoke this tool on any image URLs encountered during the search process. The model can then interpret and analyze image contents, incorporating this visual information into its context to potentially influence the trajectory of follow-up tool calls.

When the model invokes this tool, you will see it as an entry in 
chunk.tool_calls
 and 
response.tool_calls
 with the 
image_url
 as a parameter. Additionally, 
SERVER_SIDE_TOOL_VIEW_IMAGE
 will appear in 
response.server_side_tool_usage
 along with the number of times it was called when using the xAI Python SDK.

Note that enabling this feature increases token usage, as images are processed and represented as image tokens in the model's context.

Enabling this parameter for Web Search will also enable the image understanding for X Search tool if it's also included in the request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import web_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        web_search(enable_image_understanding=True),
    ],
)
chat.append(user("What is included in the image in xAI's official website?"))
# stream or sample the response...
X Search Parameters
Only Consider X Posts from Specific Handles
Use 
allowed_x_handles
 to consider X posts only from a given list of X handles. The maximum number of handles you can include is 10.

allowed_x_handles
 cannot be set together with 
excluded_x_handles
 in the same request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        x_search(allowed_x_handles=["elonmusk"]),
    ],
)
chat.append(user("What is the current status of xAI?"))
# stream or sample the response...
Exclude X Posts from Specific Handles
Use 
excluded_x_handles
 to prevent the model from including X posts from the specified handles in any X search tool invocations. The maximum number of handles you can exclude is 10.

excluded_x_handles
 cannot be set together with 
allowed_x_handles
 in the same request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        x_search(excluded_x_handles=["elonmusk"]),
    ],
)
chat.append(user("What is the current status of xAI?"))
# stream or sample the response...
Date Range
You can restrict the date range of search data used by specifying 
from_date
 and 
to_date
. This limits the data to the period from 
from_date
 to 
to_date
, including both dates.

Both fields need to be in ISO8601 format, e.g., "YYYY-MM-DD". If you're using the xAI Python SDK, the 
from_date
 and 
to_date
 fields can be passed as 
datetime.datetime
 objects.

The fields can also be used independently. With only 
from_date
 specified, the data used will be from the 
from_date
 to today, and with only 
to_date
 specified, the data used will be all data until the 
to_date
.


Python
Other

import os
from datetime import datetime
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        x_search(
            from_date=datetime(2025, 10, 1),
            to_date=datetime(2025, 10, 10),
        ),
    ],
)
chat.append(user("What is the current status of xAI?"))
# stream or sample the response...
Enable Image Understanding
Setting 
enable_image_understanding
 to true equips the agent with access to the 
view_image
 tool, allowing it to invoke this tool on any image URLs encountered during the search process. The model can then interpret and analyze image contents, incorporating this visual information into its context to potentially influence the trajectory of follow-up tool calls.

When the model invokes this tool, you will see it as an entry in 
chunk.tool_calls
 and 
response.tool_calls
 with the 
image_url
 as a parameter. Additionally, 
SERVER_SIDE_TOOL_VIEW_IMAGE
 will appear in 
response.server_side_tool_usage
 along with the number of times it was called when using the xAI Python SDK.

Note that enabling this feature increases token usage, as images are processed and represented as image tokens in the model's context.

Enabling this parameter for X Search will also enable the image understanding for Web Search tool if it's also included in the request.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        x_search(enable_image_understanding=True),
    ],
)
chat.append(user("What images are being shared in recent xAI posts?"))
# stream or sample the response...
Enable Video Understanding
Setting 
enable_video_understanding
 to true equips the agent with access to the 
view_x_video
 tool, allowing it to invoke this tool on any video URLs encountered in X posts during the search process. The model can then analyze video content, incorporating this information into its context to potentially influence the trajectory of follow-up tool calls.

When the model invokes this tool, you will see it as an entry in 
chunk.tool_calls
 and 
response.tool_calls
 with the 
video_url
 as a parameter. Additionally, 
SERVER_SIDE_TOOL_VIEW_X_VIDEO
 will appear in 
response.server_side_tool_usage
 along with the number of times it was called when using the xAI Python SDK.

Note that enabling this feature increases token usage, as video content is processed and represented as tokens in the model's context.


Python
Other

import os
from xai_sdk import Client
from xai_sdk.chat import user
from xai_sdk.tools import x_search
client = Client(api_key=os.getenv("XAI_API_KEY"))
chat = client.chat.create(
    model="grok-4-fast",  # reasoning model
    tools=[
        x_search(enable_video_understanding=True),
    ],
)
chat.append(user("What is the latest video talking about from the xAI official X account?"))
# stream or sample the response...
