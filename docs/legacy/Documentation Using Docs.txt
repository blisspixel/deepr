Here is a Python-first blueprint that (1) dynamically creates a temporary vector store, (2) bulk-uploads many documents, waits for ingestion, (3) runs a Deep Research or grounded Responses call that “refers to the docs,” and (4) logs and validates that the docs were actually used. I included clean-up and operational tips.

0) Install and init
pip install --upgrade openai tqdm

import os, time, hashlib, logging, json
from typing import Iterable, List, Optional
from openai import OpenAI
from tqdm import tqdm

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)

1) Dynamic store lifecycle (create → use → delete)

Create a short-lived vector store per research job. This keeps runs isolated, avoids cross-talk between projects, and makes cleanup straightforward. File Search parses, chunks, and indexes your files; you do not pre-embed. 
OpenAI Cookbook
+1

class EphemeralStore:
    def __init__(self, name_prefix: str = "research"):
        self.name = f"{name_prefix}-{int(time.time())}"
        self.id = None
        self.file_ids: List[str] = []

    def __enter__(self):
        vs = client.vector_stores.create(name=self.name)
        self.id = vs.id
        logging.info("Created vector store %s (%s)", self.name, self.id)
        return self

    def add_files(self, paths: Iterable[str], purpose: str = "assistants"):
        # Optional de-duplication via checksum so re-runs do not reupload unchanged files
        def sha256(path: str) -> str:
            h = hashlib.sha256()
            with open(path, "rb") as f:
                for chunk in iter(lambda: f.read(1 << 20), b""):
                    h.update(chunk)
            return h.hexdigest()

        for p in tqdm(list(paths), desc="Uploading files"):
            f = client.files.create(file=open(p, "rb"), purpose=purpose)
            self.file_ids.append(f.id)
            client.vector_stores.files.create(vector_store_id=self.id, file_id=f.id)

        logging.info("Attached %d files", len(self.file_ids))

    def wait_ingestion(self, timeout_s: int = 900, poll_s: float = 2.0):
        t0 = time.time()
        while True:
            listing = client.vector_stores.files.list(vector_store_id=self.id)
            states = [(it.id, getattr(it, "status", "completed")) for it in listing.data]
            pending = [s for s in states if s[1] != "completed"]
            if not pending:
                logging.info("Ingestion completed for %d files", len(states))
                return
            if time.time() - t0 > timeout_s:
                raise TimeoutError(f"Ingestion timeout. Pending: {pending[:3]}...")
            time.sleep(poll_s)

    def __exit__(self, exc_type, exc, tb):
        try:
            client.vector_stores.delete(self.id)
            logging.info("Deleted vector store %s", self.id)
        except Exception as e:
            logging.warning("Failed to delete vector store %s: %s", self.id, e)


Why wait: retrieval is only reliable after ingestion completes. The official examples emphasize File Search is a hosted retrieval tool you attach to Responses; waiting avoids “empty citations.” 
OpenAI Cookbook

2) “Refer to the docs” request pattern

You can scope the run to only the uploaded documents or mix in web research. Use the Responses API, attach file_search with your vector_store_id, and instruct the model to cite. 
OpenAI Cookbook

Private, doc-only research
def run_doc_only_query(store_id: str, user_query: str):
    resp = client.responses.create(
        model="gpt-4o-mini",  # or a DR-capable model if you want planning; see docs
        input=("Only use the attached documents. "
               "Answer the user query with inline citations.\n\nUser query: " + user_query),
        tools=[{"type": "file_search", "vector_store_ids": [store_id]}],
        tool_choice="required"  # force file_search usage
    )
    return resp

Deep Research plus your docs

This lets the model plan steps, browse, and also ground in your store. 
OpenAI Cookbook

def run_deep_research(store_id: str, user_query: str):
    resp = client.responses.create(
        model="o3-deep-research",  # check cookbook for current DR model names
        input=("Use the attached documents as primary sources. "
               "You may browse for corroboration. Return a structured, citation-rich report. "
               "Prefer the docs when possible.\n\nUser query: " + user_query),
        reasoning={"summary": "auto"},
        tools=[
            {"type": "web_search_preview"},
            {"type": "file_search", "vector_store_ids": [store_id]}
        ],
        tool_choice="auto"
    )
    return resp


Deep Research is accessed through Responses, plans sub-questions, calls tools like web search and code, and produces a final result with citations. 
OpenAI Cookbook

3) End-to-end job runner
def research_job(paths: List[str], query: str, deep: bool = False):
    with EphemeralStore("company-research") as es:
        es.add_files(paths)
        es.wait_ingestion()

        resp = run_deep_research(es.id, query) if deep else run_doc_only_query(es.id, query)

        # Persist raw response for audit
        payload = resp.model_dump() if hasattr(resp, "model_dump") else json.loads(resp.json())
        with open("last_response.json", "w") as f:
            json.dump(payload, f, indent=2)

        text = resp.output[-1].content[0].text
        anns = resp.output[-1].content[0].annotations or []

        logging.info("Answer length: %d chars; %d citation annotations", len(text), len(anns))
        return text, anns, payload

4) Validating that docs were actually used

You have two validation angles:

Citations and annotations on the final output. The cookbook’s Responses examples show inline citation “annotations” with title, URL, and character spans you can read and display. 
OpenAI Cookbook

def print_citations(annotations):
    for a in annotations:
        # Attributes may vary; print what you have
        print("Source:", getattr(a, "title", None) or getattr(a, "filename", None))
        print("URL:", getattr(a, "url", None))
        print("Span:", getattr(a, "start_index", "?"), getattr(a, "end_index", "?"))
        print("-" * 40)


Step traces. With Deep Research, the Responses object contains tool call items like web_search_call and retrieval steps you can parse to confirm file_search ran. The “Web Search and States” and DR cookbook show how to inspect step items. 
OpenAI Cookbook
+1

def extract_tool_calls(payload: dict, tool_type: str) -> List[dict]:
    calls = []
    for item in payload.get("output", []):
        if item.get("type", "").endswith("_call") and tool_type in item.get("type", ""):
            calls.append(item)
    return calls

# Example:
# fs_calls = extract_tool_calls(payload, "file_search")  # confirm retrieval occurred
# ws_calls = extract_tool_calls(payload, "web_search")   # confirm browsing occurred


If you prefer a visual timeline and richer telemetry, the Agents SDK includes built-in tracing and a Traces dashboard that records tool calls, LLM generations, and custom events. You can adopt it even if you orchestrate your own Responses calls. 
openai.github.io

5) Prompting patterns that work

Keep it explicit and scoped.

For doc-only tasks:

“Only use the attached vector store. If the answer is not found, say so.”

“Cite the exact file name and page number if available.”

For mixed tasks:

“Prefer attached documents. Use web search only to verify or fill gaps. Flag discrepancies.”

Ask for structure:

“Return: Executive Summary, Findings with inline citations, Open Questions, Appendix of sources.”

The Responses and DR cookbooks show examples of structured outputs and tool orchestration. 
OpenAI Cookbook
+1

6) Handling many documents efficiently

Batch uploads are fine. File Search handles parsing and retrieval. 
OpenAI Platform

Use multiple ephemeral stores if you have distinct corpora per request to keep retrieval narrow and relevant.

If you need metadata filtering, attach metadata on upload or when attaching to the store, and pass a metadata_filter in the tool config at query time. See the File Search with Responses notebook for examples. 
OpenAI Cookbook

Always wait for ingestion before querying.

7) Operational logging and audits

Keep last_response.json or a per-run log of Responses payloads. The payload contains the final answer, annotations, and tool call steps that prove which sources were consulted. 
OpenAI Cookbook

Standard HTTP logging: since the OpenAI Python SDK uses HTTPX, you can enable debug logging for HTTPX or wrap the client with an HTTPX transport to log requests and responses. Community notes show approaches for logging OpenAI+HTTPX traffic. 
TIL
+1

If you adopt the Agents SDK, enable tracing to get a first-class timeline of events. 
openai.github.io

Minimal HTTPX wire-log example:

import httpx, logging
httpx_logger = logging.getLogger("httpx")
httpx_logger.setLevel(logging.DEBUG)
httpx_logger.addHandler(logging.StreamHandler())
# The openai client uses HTTPX internally; DEBUG will log requests/responses.

8) Cleanup strategy

Delete ephemeral stores at the end of a job. The context manager above calls vector_stores.delete(...) in __exit__. If deletion fails, you log and continue.

For long-running services, add a scheduled janitor to delete stores older than N hours.

9) Failure modes and safeguards

Ingestion timeout: surface which file IDs are still pending and fail fast. Then re-run just the missing files.

No citations in output: require citations in the prompt and verify annotations exist; if empty, re-prompt with “If you cannot find evidence in the attached docs, state ‘Insufficient evidence in attached sources’.”

Tool selection drift: set tool_choice="required" to force file_search for doc-only runs. 
OpenAI Cookbook

Mixing tools: check the step log to ensure file_search actually ran at least once when expected. If not, retry with a stronger instruction like “You must query file_search before answering.”

10) Example: run now
if __name__ == "__main__":
    docs = [
        "company_docs/handbook.pdf",
        "company_docs/policies.docx",
        "company_docs/notes.txt",
    ]
    query = "Refer to the docs. List mandatory approval steps for vendor onboarding. Include inline citations."

    text, anns, payload = research_job(docs, query, deep=False)
    print(text)
    print_citations(anns)

11) References to keep current

File Search with Responses API, including creating a vector store, attaching files, and querying with citations. 
OpenAI Cookbook
+1

Deep Research overview and Python examples via the Responses API. 
OpenAI Cookbook
+1

Responses API orchestration and tool usage, including web search and structured outputs. 
OpenAI Cookbook
+1

Tracing runs with the Agents SDK for debugging and validation. 
openai.github.io