Assistants migration guide
Migrate from the Assistants API to the Responses API.
After achieving feature parity in the Responses API, we've deprecated the Assistants API. It will shut down on August 26, 2026. Follow the guidance below to update your integration. Learn more.

We're moving from the Assistants API to the new Responses API for a simpler and more flexible mental model.

Responses are simpler‚Äîsend input items and get output items back. With the Responses API, you also get better performance and new features like deep research, MCP, and computer use. This change also lets you manage conversations instead of passing back previous_response_id.

What's changed?
Before	Now	Why?
Assistants	Prompts	Prompts hold configuration (model, tools, instructions) and are easier to version and update
Threads	Conversations	Streams of items instead of just messages
Runs	Responses	Responses send input items or use a conversation object and receive output items; tool call loops are explicitly managed
Run steps	Items	Generalized objects‚Äîcan be messages, tool calls, outputs, and more
From assistants to prompts
Assistants were persistent API objects that bundled model choice, instructions, and tool declarations‚Äîcreated and managed entirely through the API. Their replacement, prompts, can only be created in the dashboard, where you can version them as you develop your product.

Why this is helpful
Portability and versioning: You can snapshot, review, diff, and roll back prompt specs. You can also version a prompt, so your code can just point the latest version.
Separation of concerns: Your application code now handles orchestration (history pruning, tool loop, retries) while your prompt focuses on high‚Äëlevel behavior and constraints (system guidance, tool availability, structured output schema, temperature defaults).
Realtime compatibility: The same prompt configuration can be reused when you connect through the Realtime API, giving you a single definition of behavior across chat, streaming, and low‚Äëlatency interactive sessions.
Tool and output consistency: Using prompts, every Responses or Realtime session you start inherits a consistent contract because prompts encapsulate tool schemas and structured output expectations.
Practical migration steps
Identify each existing Assistant‚Äôs instruction + tool bundle.
In the dashboard, recreate that bundle as a named prompt.
Store the prompt ID (or its exported spec) in source control so application code can refer to a stable identifier.
During rollout, run A/B tests by swapping prompt IDs‚Äîno need to create or delete assistant objects programmatically.
Think of a prompt as a versioned behavioral profile to plug into either Responses or Realtime API.

From threads to conversations
A thread was a collection of messages stored server-side. Threads could only store messages. Conversations store items, which can include messages, tool calls, tool outputs, and other data.

Request example
Thread object
thread = openai.beta.threads.create(
  messages=[{"role": "user", "content": "what are the 5 Ds of dodgeball?"}],
  metadata={"user_id": "peter_le_fleur"},
)
Conversation object
conversation = openai.conversations.create(
  items=[{"role": "user", "content": "what are the 5 Ds of dodgeball?"}],
  metadata={"user_id": "peter_le_fleur"},
)
Response example
Thread object
{
"id": "thread_CrXtCzcyEQbkAcXuNmVSKFs1",
"object": "thread",
"created_at": 1752855924,
"metadata": {
  "user_id": "peter_le_fleur"
},
"tool_resources": {}
}
Conversation object
{
"id": "conv_68542dc602388199a30af27d040cefd4087a04b576bfeb24",
"object": "conversation",
"created_at": 1752855924,
"metadata": {
	"user_id": "peter_le_fleur"
}
}
From runs to responses
Runs were asynchronous processes that executed against threads. See the example below. Responses are simpler: provide a set of input items to execute, and get a list of output items back.

Responses are designed to be used alone, but you can also use them with prompt and conversation objects for storing context and configuration.

Request example
Run object
thread_id = "thread_CrXtCzcyEQbkAcXuNmVSKFs1"
assistant_id = "asst_8fVY45hU3IM6creFkVi5MBKB"

run = openai.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant.id)

while run.status in ("queued", "in_progress"):
  time.sleep(1)
  run = openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)
Response object
response = openai.responses.create(
  model="gpt-4.1",
  input=[{"role": "user", "content": "What are the 5 Ds of dodgeball?"}]
  conversation: "conv_689667905b048191b4740501625afd940c7533ace33a2dab"
)
Response example
Run object
{
"id": "run_FKIpcs5ECSwuCmehBqsqkORj",
"assistant_id": "asst_8fVY45hU3IM6creFkVi5MBKB",
"cancelled_at": null,
"completed_at": 1752857327,
"created_at": 1752857322,
"expires_at": null,
"failed_at": null,
"incomplete_details": null,
"instructions": null,
"last_error": null,
"max_completion_tokens": null,
"max_prompt_tokens": null,
"metadata": {},
"model": "gpt-4.1",
"object": "thread.run",
"parallel_tool_calls": true,
"required_action": null,
"response_format": "auto",
"started_at": 1752857324,
"status": "completed",
"thread_id": "thread_CrXtCzcyEQbkAcXuNmVSKFs1",
"tool_choice": "auto",
"tools": [],
"truncation_strategy": {
  "type": "auto",
  "last_messages": null
},
"usage": {
  "completion_tokens": 130,
  "prompt_tokens": 34,
  "total_tokens": 164,
  "prompt_token_details": {
    "cached_tokens": 0
  },
  "completion_tokens_details": {
    "reasoning_tokens": 0
  }
},
"temperature": 1.0,
"top_p": 1.0,
"tool_resources": {},
"reasoning_effort": null
}
Response object
{
"id": "resp_687a7b53036c819baad6012d58b39bcb074adcd9e24850fc",
"created_at": 1752857427,
"conversation": {
  "id": "conv_689667905b048191b4740501625afd940c7533ace33a2dab"
},
"error": null,
"incomplete_details": null,
"instructions": null,
"metadata": {},
"model": "gpt-4.1-2025-04-14",
"object": "response",
"output": [
  {
    "id": "msg_687a7b542948819ba79e77e14791ef83074adcd9e24850fc",
    "content": [
      {
        "annotations": [],
        "text": "The \"5 Ds of Dodgeball\" are a humorous set of rules made famous by the 2004 comedy film **\"Dodgeball: A True Underdog Story.\"** In the movie, dodgeball coach Patches O‚ÄôHoulihan teaches these basics to his team. The **5 Ds** are:

1. **Dodge**
2. **Duck**
3. **Dip**
4. **Dive**
5. **Dodge** (yes, dodge is listed twice for emphasis!)

In summary:  
> **‚ÄúIf you can dodge a wrench, you can dodge a ball!‚Äù**

These 5 Ds are not official competitive rules, but have become a fun and memorable pop culture reference for the sport of dodgeball.",
        "type": "output_text",
        "logprobs": []
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  }
],
"parallel_tool_calls": true,
"temperature": 1.0,
"tool_choice": "auto",
"tools": [],
"top_p": 1.0,
"background": false,
"max_output_tokens": null,
"previous_response_id": null,
"reasoning": {
  "effort": null,
  "generate_summary": null,
  "summary": null
},
"service_tier": "scale",
"status": "completed",
"text": {
  "format": {
    "type": "text"
  }
},
"truncation": "disabled",
"usage": {
  "input_tokens": 17,
  "input_tokens_details": {
    "cached_tokens": 0
  },
  "output_tokens": 150,
  "output_tokens_details": {
    "reasoning_tokens": 0
  },
  "total_tokens": 167
},
"user": null,
"max_tool_calls": null,
"store": true,
"top_logprobs": 0
}
Migrating your integration
Follow the migration steps below to move from the Assistants API to the Responses API, without losing any feature support.

1. Create prompts from your assistants
Identify the most important assistant objects in your application.
Find these in the dashboard and click Create prompt.
This will create a prompt object out of each existing assistant object.

2. Move new user chats over to conversations and responses
We will not provide an automated tool for migrating Threads to Conversations. Instead, we recommend migrating new user threads onto conversations and backfilling old ones as necessary.

Here's an example for how you might backfill a thread:

thread_id = "thread_EIpHrTAVe0OzoLQg3TXfvrkG"

for page in openai.beta.threads.messages.list(thread_id=thread_id, order="asc").iter_pages():
    messages += page.data

items = []
for m in messages:
    item = {"role": m.role}
    item_content = []

    for content in m.content:
        match content.type:
            case "text":
                item_content_type = "input_text" if m.role == "user" else "output_text"
                item_content += [{"type": item_content_type, "text": content.text.value}]
            case "image_url":
                item_content + [
                    {
                        "type": "input_image",
                        "image_url": content.image_url.url,
                        "detail": content.image_url.detail,
                    }
                ]

    item |= {"content": item_content}
    items.append(item)

# create a conversation with your converted items
conversation = openai.conversations.create(items=items)
Comparing full examples
Here‚Äôs a few simple examples of integrations using both the Assistants API and the Responses API so you can see how they compare.

User chat app
Assistants API
Responses API
thread = openai.threads.create()

@app.post("/messages")
async def message(message: Message):
openai.beta.threads.messages.create(
	role="user",
	content=message.content
)

run = openai.beta.threads.runs.create(
	assistant_id=os.getenv("ASSISTANT_ID"),
	thread_id=thread.id
)
while run.status in ("queued", "in_progress"):
  await asyncio.sleep(1)
  run = openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)

messages = openai.beta.threads.messages.list(
	order="desc", limit=1, thread_id=thread.id
)

return { "content": messages[-1].content } Comparing full examples
Here‚Äôs a few simple examples of integrations using both the Assistants API and the Responses API so you can see how they compare.

User chat app
Assistants API
Responses API
conversation = openai.conversations.create()

@app.post("/messages")
async def message(message: Message):
response = openai.responses.create(
	prompt={ "id": os.getenv("PROMPT_ID") },
	input=[{ "role": "user", "content": message.content }]
)

return { "content": response.output_text }' 
You said:
does openai gpt 5 support assistant api
ChatGPT said:
No ‚Äî as of now, GPT‚Äë5 is not supported via the Assistants API from OpenAI.

Here are the key details:

Many developers report error messages like:

‚ÄúThe requested model ‚Äògpt-5‚Äô cannot be used with the Assistants API.‚Äù 
OpenAI Developer Community
+1

Community posts note that GPT-5 appears in model listings for other endpoints (e.g., completions/responses) but not yet in the Assistants API. 
OpenAI Developer Community
+1

According to OpenAI‚Äôs documentation, the Assistants API supports model usage, threads, tools, etc. ‚Äî but it does not yet clearly indicate GPT-5 availability via that endpoint. 
OpenAI Help Center
+1

One migration-guide blog states that GPT-5 (and some other newer models) are exclusive to the newer ‚ÄúResponses API‚Äù (or other new endpoints), and that the Assistants API is expected to be deprecated by 2026. 
CalStudio

What you can do
You can use GPT-5 via other endpoints (e.g., Completions or Responses APIs) as supported by your account.

If you intend to build a conversational assistant and need GPT-5, you‚Äôll likely need to transition to the new endpoint suggested by OpenAI (the ‚ÄúResponses API‚Äù or equivalent) rather than the Assistants API.

Keep an eye on OpenAI‚Äôs official docs for when/if GPT-5 becomes available in the Assistants API ‚Äî access is apparently being rolled out gradually and may depend on region/account. 
OpenAI Developer Community
+1

If you like, I can check the latest status (account-region specific) for you and see when GPT-5 support for Assistants API might be available.

You said:
so does gpt 5 suppport RAG and tools then?
ChatGPT said:
Yes ‚Äî GPT‚Äë5 does support Retrieval-Augmented Generation (RAG) workflows and tool-calling features, but with some caveats and implementation nuances.

Here‚Äôs a breakdown:

‚úÖ What it supports
According to the official OpenAI docs, GPT-5 is described as being trained for ‚Äúlong context and tool calling.‚Äù 
OpenAI Platform

There are several public write-ups and blog posts showing GPT-5 being used in RAG pipelines (embedding ‚Üí vector search ‚Üí feeding context ‚Üí generation) in enterprise settings. 
News from generation RAG
+3
Clarifai
+3
n8n
+3

In the OpenAI ‚ÄúNew Params and Tools‚Äù cookbook for GPT-5, mentions include support for free-form function calling and other newer parameters. 
OpenAI Cookbook

‚ö†Ô∏è Where the nuance lies
Just because ‚Äútool-calling‚Äù is supported doesn‚Äôt mean every tool or every workflow will be automatically enabled or identical to earlier models. For example, one thread notes that parameters like reasoning_effort may or may not be supported depending on which API/endpoint you're using. 
Microsoft Learn

The integration specifics (which endpoint to use: Chat Completions vs Responses API), parameter support, access tiers, model variants (-mini, -nano versions) can vary. In the GPT-5 cookbook, it states:

‚ÄúSupported Models: gpt-5, gpt-5-mini, gpt-5-nano. Supported API Endpoints: Responses API, Chat Completions API.‚Äù 
OpenAI Cookbook

For RAG specifically: OpenAI docs around ‚ÄúRAG and Semantic Search for GPTs‚Äù discuss how retrieval from uploaded files or vector stores is enabled when knowledge retrieval is turned on. 
OpenAI Help Center

But just because a model supports RAG‚Äêstyle workflows doesn‚Äôt mean your particular API key / account / region will have all features yet.

üß≠ Practical advice
If you are planning to build using RAG + tool-calling with GPT-5, consider:

Verify you have access to the specific GPT-5 model variant (gpt-5, gpt-5-mini, etc) in your account.

Check which endpoint you should be using. The ‚ÄúResponses API‚Äù may offer more advanced control for GPT-5.

Test your tool calling and retrieval pipeline (vector store ‚Üí embedding ‚Üí retrieval ‚Üí prompt augmentation) with a small example to verify everything works end-to-end.

Pay attention to parameters that may differ (e.g., max_completion_tokens instead of max_tokens in some cases) as pointed out in one Q&A thread for Azure OpenAI with GPT-5. 
Microsoft Learn

Be aware of latency, cost, and context window trade-offs if you‚Äôre retrieving large documents or many passages as part of RAG.

If you like, I can check for your account region whether GPT-5‚Äôs full tool-calling and RAG features are enabled (or if there are any restrictions). Would you like me to do that?

You said:
give code specific examples for rag with GpT5 then
ChatGPT said:

Thought for 25s
Below are compact, working patterns you can adapt. They show two common RAG styles with GPT-5:

classic RAG (retrieve ‚Üí stuff ‚Üí generate), and

tool-calling RAG (the model asks you to call your retriever ‚Äúfunction,‚Äù you run it, then feed results back).

I‚Äôm using the Chat Completions API for stability. Swap to the Responses API if that is your standard.

1) Classic RAG: retrieve ‚Üí stuff ‚Üí generate
Python
# pip install openai faiss-cpu tiktoken
import os
from openai import OpenAI
import faiss
import numpy as np

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

EMBED_MODEL = "text-embedding-3-large"
GEN_MODEL = "gpt-5"

# 1) Build or load your vector index (one-time). Here we demo a tiny in-memory index.
docs = [
    {"id": "a1", "text": "Pueo builds data-driven security solutions focused on ..."},
    {"id": "b2", "text": "RAG combines retrieval from a knowledge base with generation to improve factuality."},
    {"id": "c3", "text": "To reduce hallucinations, cite sources and include quoted evidence where possible."},
]

embs = client.embeddings.create(model=EMBED_MODEL, input=[d["text"] for d in docs]).data
X = np.vstack([np.array(e.embedding, dtype="float32") for e in embs])
index = faiss.IndexFlatIP(X.shape[1])
faiss.normalize_L2(X)
index.add(X)

# Helper to search
def search(query, k=3):
    q = client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding
    q = np.array(q, dtype="float32").reshape(1, -1)
    faiss.normalize_L2(q)
    scores, idxs = index.search(q, k)
    return [(docs[i]["id"], docs[i]["text"], float(scores[0][j])) for j, i in enumerate(idxs[0])]

# 2) User question
question = "Explain RAG simply and cite the most relevant snippets."

# 3) Retrieve top passages
hits = search(question, k=3)
context = "\n\n".join([f"[{doc_id}] {text}" for doc_id, text, _ in hits])

# 4) Ask GPT-5 with grounded context
messages = [
    {"role": "system", "content": "You are a careful assistant. Use only the provided context to answer and cite [doc_id]s."},
    {"role": "user", "content": f"Question: {question}\n\nContext:\n{context}\n\nAnswer with citations like [a1]."}
]

resp = client.chat.completions.create(
    model=GEN_MODEL,
    messages=messages,
    temperature=0.2,
)

print(resp.choices[0].message.content)
Node.js
// npm i openai faiss-node  (or use your own store); faiss-node is optional
import OpenAI from "openai";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const EMBED_MODEL = "text-embedding-3-large";
const GEN_MODEL = "gpt-5";

// Assume you already have a vector store function getTopK(query, k) that returns [{id, text, score}, ...]
async function getTopK(query, k = 3) {
  const emb = await openai.embeddings.create({ model: EMBED_MODEL, input: [query] });
  const qvec = emb.data[0].embedding;
  // Replace with your own similarity search; return mock data for brevity:
  return [
    { id: "b2", text: "RAG combines retrieval from a knowledge base with generation to improve factuality.", score: 0.92 },
    { id: "c3", text: "To reduce hallucinations, cite sources and include quoted evidence where possible.", score: 0.88 },
  ];
}

async function answer(question) {
  const hits = await getTopK(question, 3);
  const context = hits.map(h => `[${h.id}] ${h.text}`).join("\n\n");

  const messages = [
    { role: "system", content: "Use only the provided context to answer and include [doc_id] citations." },
    { role: "user", content: `Question: ${question}\n\nContext:\n${context}\n\nAnswer with citations.` }
  ];

  const resp = await openai.chat.completions.create({
    model: GEN_MODEL,
    messages,
    temperature: 0.2
  });

  console.log(resp.choices[0].message.content);
}

answer("Explain RAG simply and cite the most relevant snippets.");
Tips for classic RAG

Keep retrieved chunks small and relevant (200‚Äì400 tokens).

Add a system rule like ‚ÄúIf the answer is not in context, say you don‚Äôt know.‚Äù

Consider a second ‚Äúverification‚Äù pass where the model checks its own answer against the context.

2) Tool-calling RAG: let GPT-5 request retrieval
In this pattern the model decides when to call your retriever. You register a function schema, run retrieval when the model asks, then send the tool‚Äôs output back and get the final grounded answer.

Python
import os, json
from openai import OpenAI
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

GEN_MODEL = "gpt-5"
EMBED_MODEL = "text-embedding-3-large"

# Dummy retriever for illustration; replace with your vector DB call
def search_knowledge_base(query: str, top_k: int = 5):
    # ... run your similarity search and return [{id, text, score}]
    return [
        {"id": "b2", "text": "RAG combines retrieval with generation to improve factuality.", "score": 0.92},
        {"id": "c3", "text": "Cite sources and include quoted evidence where possible.", "score": 0.88},
    ]

tools = [
    {
        "type": "function",
        "function": {
            "name": "search_knowledge_base",
            "description": "Retrieve the most relevant passages from the org knowledge base.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Natural language query."},
                    "top_k": {"type": "integer", "minimum": 1, "maximum": 20, "default": 5}
                },
                "required": ["query"]
            },
        },
    }
]

user_q = "What is RAG and why does it reduce hallucinations? Cite sources."

# Step 1: let GPT-5 decide whether to call the retriever
first = client.chat.completions.create(
    model=GEN_MODEL,
    messages=[{"role": "user", "content": user_q}],
    tools=tools,
    tool_choice="auto",
    temperature=0.2,
)

msg = first.choices[0].message

# Step 2: handle tool calls (if any)
tool_messages = []
if msg.tool_calls:
    for call in msg.tool_calls:
        if call.type == "function" and call.function.name == "search_knowledge_base":
            args = json.loads(call.function.arguments or "{}")
            results = search_knowledge_base(args.get("query",""), args.get("top_k", 5))
            # Send results back as a tool message tied to the tool_call_id
            tool_messages.append({
                "role": "tool",
                "tool_call_id": call.id,
                "content": json.dumps({"results": results})
            })

# Step 3: ask GPT-5 to produce the grounded answer
final = client.chat.completions.create(
    model=GEN_MODEL,
    messages=[
        {"role": "user", "content": user_q},
        msg,  # the assistant message with tool_calls
        *tool_messages
    ],
    temperature=0.2,
)

print(final.choices[0].message.content)
Node.js
import OpenAI from "openai";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const GEN_MODEL = "gpt-5";

const tools = [
  {
    type: "function",
    function: {
      name: "search_knowledge_base",
      description: "Retrieve relevant passages from the knowledge base.",
      parameters: {
        type: "object",
        properties: {
          query: { type: "string", description: "User's natural language query." },
          top_k: { type: "integer", minimum: 1, maximum: 20, default: 5 }
        },
        required: ["query"]
      }
    }
  }
];

// Your retriever
async function search_knowledge_base({ query, top_k = 5 }) {
  // Call your vector DB here. Demo payload:
  return [
    { id: "b2", text: "RAG combines retrieval with generation to improve factuality.", score: 0.92 },
    { id: "c3", text: "Cite sources and include quoted evidence where possible.", score: 0.88 },
  ];
}

async function ragToolCalling(userQuestion) {
  const first = await openai.chat.completions.create({
    model: GEN_MODEL,
    messages: [{ role: "user", content: userQuestion }],
    tools,
    tool_choice: "auto",
    temperature: 0.2
  });

  const msg = first.choices[0].message;

  let toolMsgs = [];
  if (msg.tool_calls?.length) {
    for (const call of msg.tool_calls) {
      if (call.type === "function" && call.function.name === "search_knowledge_base") {
        const args = JSON.parse(call.function.arguments || "{}");
        const results = await search_knowledge_base({ query: args.query, top_k: args.top_k });
        toolMsgs.push({
          role: "tool",
          tool_call_id: call.id,
          content: JSON.stringify({ results })
        });
      }
    }
  }

  const final = await openai.chat.completions.create({
    model: GEN_MODEL,
    messages: [
      { role: "user", content: userQuestion },
      msg,          // includes tool_calls
      ...toolMsgs   // tool outputs
    ],
    temperature: 0.2
  });

  console.log(final.choices[0].message.content);
}

ragToolCalling("What is RAG and why does it reduce hallucinations? Cite sources.");
Tips for tool-calling RAG

Return structured fields from your retriever: id, text, source_url, score.

In your system prompt, require explicit citations and refusal when evidence is absent.

You can chain tools: e.g., retriever ‚Üí web fetcher ‚Üí summarizer.

3) Optional upgrades
Rerank before stuffing. Use the model or a lightweight cross-encoder to rerank your top-K and keep 3‚Äì6 best chunks.

Use ‚Äúquote and cite‚Äù prompting. Require answers to include short quotes from the retrieved passages with [doc_id] tags.

Add a verification pass. After generation, ask GPT-5 to verify each claim against the provided snippets and mark any unsupported statements.

If you want, tell me your preferred stack (Pinecone, PGVector, Vespa, Weaviate, Elastic, etc.) and I will tailor these snippets to that SDK with batch indexing and streaming.

