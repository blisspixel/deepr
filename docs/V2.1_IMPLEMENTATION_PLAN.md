# v2.1 Implementation Plan: Context Chaining

## What We're Building

Multi-phase research where Phase N results feed into Phase N+1 prompts, culminating in a synthesis report that integrates all findings.

## What We Already Have

### Documentation (docs/research and documentation/)
- OpenAI Deep Research API docs (complete)
- GPT-5 prompting guide (complete)
- Azure integration patterns (complete)
- Responses API documentation (complete)

### Working Code
- `ResearchPlanner` service (generates phased plans)
- `deepr prep plan` command (creates plan, saves to file)
- Queue, storage, providers all working
- Research agent polling

## What We Need to Build

### 1. Context Injection Service

**Purpose:** Summarize Phase N results for Phase N+1 context

**Implementation:**
```python
# deepr/services/context_builder.py

class ContextBuilder:
    """Builds context from prior research for subsequent phases."""

    async def summarize_research(self, job_id: str) -> str:
        """
        Summarize a research report for use as context.

        Input: Full research report (could be 10+ pages)
        Output: Concise summary (500-1000 tokens) with key findings
        """
        pass

    async def build_phase_context(self, task: dict, completed_tasks: list) -> str:
        """
        Build context string for a task based on dependencies.

        task: Current task with 'depends_on' field
        completed_tasks: List of completed research jobs

        Returns: Context string to inject into prompt
        """
        pass
```

**Strategy:**
- Use GPT-4 (cheap, fast) to summarize research reports
- Extract key findings, insights, data points
- Format as structured context for next phase
- Keep under 2K tokens per report to avoid context limit

### 2. Batch Execution Logic

**Purpose:** Execute research plan with phase dependencies

**Implementation:**
```python
# deepr/services/batch_executor.py

class BatchExecutor:
    """Executes multi-phase research campaigns."""

    async def execute_plan(self, plan_file: str, approved_tasks: list):
        """
        Execute research plan with dependencies.

        1. Load plan from file
        2. Execute Phase 1 tasks in parallel
        3. Wait for Phase 1 completion
        4. Build context from Phase 1 results
        5. Execute Phase 2 with Phase 1 context
        6. Repeat for all phases
        """
        pass

    async def execute_phase(self, phase: int, tasks: list, context: dict):
        """
        Execute all tasks in a phase.

        For each task:
        1. Build context from dependencies
        2. Inject context into prompt
        3. Submit to queue
        4. Track job ID
        """
        pass
```

**Workflow:**
```
Phase 1: Submit tasks 1,2,3 (parallel)
  ↓ (wait for all to complete)
Phase 2:
  - Summarize task 1,2 results
  - Inject summaries into task 4 prompt
  - Submit task 4
  - (repeat for task 5 with dependencies)
  ↓ (wait)
Phase 3:
  - Summarize ALL previous results
  - Inject into synthesis prompt
  - Submit synthesis task
```

### 3. CLI Commands

**prep review**
```bash
deepr prep review [plan-file]

Shows plan, lets user:
- Approve/reject tasks
- Edit prompts
- Adjust dependencies
- Change cost limits

Saves approved plan for execution.
```

**prep execute**
```bash
deepr prep execute [plan-file] [--phase N]

Executes approved plan:
- Shows progress by phase
- Displays context being built
- Updates as tasks complete
- Returns campaign summary when done
```

**prep status**
```bash
deepr prep status [campaign-id]

Shows campaign execution status:
- Tasks by phase
- Completed/in-progress/pending
- Total cost so far
- Estimated time remaining
```

## Research We're Running Right Now

**Job:** "Research best practices for context injection in multi-step LLM workflows"

**Will answer:**
- How to summarize LLM outputs for context?
- Token limits for context injection?
- Effective prompt patterns for chaining?
- Summarization strategies?

**When complete:** Save to `docs/research and documentation/context_chaining_best_practices.md`

**Use for:** Implementing ContextBuilder service

## Implementation Order

### Week 1: Core Services
1. **ContextBuilder service** - Summarization logic
2. **BatchExecutor service** - Phase execution
3. Unit tests for both

### Week 2: CLI Integration
1. **prep review command** - Approve/edit plans
2. **prep execute command** - Run campaigns
3. **prep status command** - Track progress

### Week 3: Testing & Polish
1. End-to-end test with real campaign
2. Cost tracking for campaigns
3. Error handling and edge cases
4. Documentation updates

## Success Criteria

**v2.1 is complete when:**

User can:
```bash
# Generate plan
deepr prep plan "Build AI code review tool" --topics 5

# Review and approve
deepr prep review
# (interactive: approve tasks 1-5, reject task 6)

# Execute
deepr prep execute

# Track progress
deepr prep status
Phase 1: ✓ Complete (3 tasks, $1.52, 18 min)
Phase 2: → In progress (1/2 tasks complete)
Phase 3: ⋯ Pending

# Get results when done
deepr research result <synthesis-job-id>
# (Shows comprehensive report integrating all findings)
```

**And:** Context from Phase 1 is clearly visible in Phase 2 results

**And:** Synthesis report references specific findings from all phases

## What We DON'T Need

- More API documentation (we have it)
- More research on Deep Research itself (we understand it)
- Complex distributed systems (local queue is fine)
- Web interface (CLI works)

## What We MIGHT Need

Research questions that could emerge:
- Token limits causing context truncation?
  → Research: "How to compress research findings for context"

- Synthesis quality issues?
  → Research: "Best practices for multi-document synthesis with LLMs"

- Cost optimization?
  → Research: "Strategies to reduce cost in chained LLM workflows"

**Approach:** Run these as needed using Deepr itself (dogfooding)

## The Meta Point

**We're using Deepr to build Deepr.**

When we hit implementation questions, we:
1. Formulate research question
2. Submit to Deepr
3. Get comprehensive answer
4. Implement based on findings
5. Document the research

This is both:
- Validation that the tool works
- Generation of implementation guidance
- Documentation of our process

**Wild but perfect.**

## Next Steps

1. Wait for context chaining research to complete (~10 min)
2. Review findings
3. Implement ContextBuilder based on recommendations
4. Build BatchExecutor
5. Wire up CLI commands
6. Test end-to-end
7. Ship v2.1

**Timeline:** 1-2 weeks of focused work

**Confidence:** High - core infrastructure works, just need orchestration layer
